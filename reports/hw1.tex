\documentclass[10pt]{article}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Homework 1: N-gram Language Models}
\author{Yu Feng}
\date{2-16-2015}

\begin{document}
\maketitle

\section{Introduction}
In this homework, my task is to produce both a ``backward" bigram model and a bidirectional model based on the sample code of a normal bigram model. 
 
\section{Algorithm}\label{sec:alg}

\subsection{Backward Bigram Model}

The algorithm for the backward bigram model is almost identical to the normal bigram model except for the model direction. So what I need to do is to swap the model direction for both the training and testing data and then invoke the original corresponding functions. 


\subsection{Bidirectional Bigram Model}

Here are the steps to build the bidirectional bigram model:

Step 1: Create an instance of the bidirectional model which has references to the instances of the bigram model and the backward bigram model;

Step 2: Train each model separately by invoking its own training method using the given data;

Step 3: When determining the probability of each word($\alpha_i$) using the bidirectional model, linearly interpolate the predicted results of the bigram model and the backward bigram model based on the following equation:
\[
  \alpha_{i} = log(\lambda_{1} * \beta_{i} + \lambda_{2} * \gamma_{i})
\] 
Here, $\beta_{i}$ and $\gamma_{i}$ are the probability predicted by the bigram model and the backward bigram model, respectively. $\lambda_{1}$, $\lambda_{2}$ are their corresponding weight. I weight them equally in the experiment. 

Step 4: The probability for the entire sentence will be the sum of all its words:
\[
   \Psi = \sum_{i=1}^{n}{ \alpha_{i}}
\]
 where $n$ is the number of words in current sentence.
\section{Experiments}
I implemented the algorithms described in section~\ref{sec:alg} and tested them on the atis, wsj and brown corpora.  Table~\ref{table:train} and Table~\ref{table:test} shows the overall results for word perplexity.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|l|l|}  \hline
POS Data    & Bigram & Backward & Bidirectional \\ \hline
atis        & 10.592 & 11.636 & 7.235  \\ \hline
wsj         & 88.890 & 86.660 & 46.514  \\  \hline
brown       & 113.360 & 110.783 & 61.469  \\  \hline
\end{tabular}
\caption{\small Comparison of Word Perplexity among three models on the training data from the atis, wsj and brown corpora.}\label{table:train}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|l|l|}  \hline
POS Data    & Bigram & Backward & Bidirectional \\ \hline
atis        & 24.054 & 27.161 & 12.700  \\ \hline
wsj         & 275.118 & 266.352 & 126.113  \\  \hline
brown       & 310.667 & 299.686 & 167.487  \\  \hline
\end{tabular}
\caption{\small Comparison of Word Perplexity among three models on the testing data from the atis, wsj and brown corpora.}\label{table:test}
\end{center}
\end{table}

The results show that the backward bigram model outperforms the normal bigram model in the \emph{wsj} and \emph{brown} corpora, but worse in the \emph{atis} corpora; Meanwhile, bidirectional bigram model yields the best word perplexity across all POS data sets.


\section{Discussion}

\emph{RQ1:How does the ``Word Perplexity" of the backward bigram model (for both training and test data) compare to the normal model?}
\\

Intuitively speaking, given a large corpora, it's hard to tell which model is better since the only difference is the model direction. I think their performances are related to the 
characteristics of the corpora: the backward model may be better in some corpora but worse otherwise. The results of table~\ref{table:train} and table~\ref{table:test} are consistent with this point. But we still need more data and different parameters to verify this point.
\\

\emph{RQ2:How does the ``Word Perplexity" of the bidirectional model (for both training and test data) compare to both the backward model and the normal model?}
\\

Table~\ref{table:train} and Table~\ref{table:test} show that the bidirectional model is strictly better than both the backward model and the normal model in terms of "Word Perplexity". This is also reasonable because the bidirectional algorithm is taking advantage of the knowledge from both sides. When predicting each token, the bidirectional model will have constraints from forward and backward thus the results outperform any of those two. 

\end{document}
